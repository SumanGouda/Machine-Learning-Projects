{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2df36be-27af-4ff9-a26c-331a5de61876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized columns: ['sample_id', 'setting', 'total_carbon_wt_pct', 'graphite_wt_pct', 'carbon_black_wt_pct', 'resin_wt_pct', 'pitch_wt_pct', 'graphene_wt_pct', 'cnt_wt_pct', 'gnp_wt_pct', 'antioxidant_wt_pct', 'mgo_purity_pct', 'd50_micron', 'porosity_pct', 'density_g_cm3', 'thermal_conductivity_w_mk', 'oxidation_mass_loss_pct', 'oxidation_penetration_mm', 'hot_mor_mpa', 'slag_contact_angle_deg', 'residual_strength_pct_after_shock', 'dominant_carbon_source']\n",
      "\n",
      "Model will be trained on these 11 features:\n",
      " ['total_carbon_wt_pct', 'graphite_wt_pct', 'carbon_black_wt_pct', 'resin_wt_pct', 'pitch_wt_pct', 'graphene_wt_pct', 'cnt_wt_pct', 'gnp_wt_pct', 'antioxidant_wt_pct', 'mgo_purity_pct', 'd50_micron']\n",
      "\n",
      "Running RandomizedSearchCV...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Test Metrics for the New Model:\n",
      "                                          r2       rmse\n",
      "porosity_pct                       0.310290   1.542902\n",
      "density_g_cm3                      0.232537   0.063418\n",
      "thermal_conductivity_w_mk          0.731024   2.852611\n",
      "oxidation_mass_loss_pct            0.357889   0.364167\n",
      "oxidation_penetration_mm           0.694087   1.412219\n",
      "hot_mor_mpa                        0.842160  15.577196\n",
      "slag_contact_angle_deg             0.715474   6.330737\n",
      "residual_strength_pct_after_shock  0.885373   8.157239\n",
      "\n",
      "Generating and organizing diagnostic plots...\n",
      "Saving residual plots to: model_outputs\\residual_plots\n",
      "Saving learning curve plots to: model_outputs\\learning_curves\n",
      "Generating learning curve for: porosity_pct...\n",
      "Generating learning curve for: density_g_cm3...\n",
      "Generating learning curve for: thermal_conductivity_w_mk...\n",
      "Generating learning curve for: oxidation_mass_loss_pct...\n",
      "Generating learning curve for: oxidation_penetration_mm...\n",
      "Generating learning curve for: hot_mor_mpa...\n",
      "Generating learning curve for: slag_contact_angle_deg...\n",
      "Generating learning curve for: residual_strength_pct_after_shock...\n",
      "\n",
      "âœ… All outputs, plots and the model are saved in the 'model_outputs' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----- CONFIG -----\n",
    "DATA_PATH = \"Dataset for model feed - MgO C.xlsx\"\n",
    "OUT_DIR = \"model_outputs\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "CV_FOLDS = 5\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- LOAD -----\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{DATA_PATH}' was not found.\")\n",
    "    exit()\n",
    "\n",
    "# ----- STANDARDIZE COLUMN NAMES -----\n",
    "def standardize_columns(df):\n",
    "    cols = df.columns.str.strip().str.lower()\n",
    "    cols = cols.str.replace('%', '_pct', regex=False)\n",
    "    cols = cols.str.replace(' ', '_', regex=False)\n",
    "    cols = cols.str.replace('-', '_', regex=False)\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "df = standardize_columns(df)\n",
    "print(\"Standardized columns:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "# ----- DEFINE FEATURES AND TARGETS -----\n",
    "target_cols = [\n",
    "    'porosity_pct', 'density_g_cm3', 'thermal_conductivity_w_mk',\n",
    "    'oxidation_mass_loss_pct', 'oxidation_penetration_mm', 'hot_mor_mpa',\n",
    "    'slag_contact_angle_deg', 'residual_strength_pct_after_shock'\n",
    "]\n",
    "id_cols = ['sample_id', 'setting', 'dominant_carbon_source']\n",
    "feature_cols = [c for c in df.columns if c not in target_cols + id_cols]\n",
    "print(f\"\\nModel will be trained on these {len(feature_cols)} features:\\n\", feature_cols)\n",
    "\n",
    "# ----- STRATIFIED SPLIT -----\n",
    "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True, stratify=df['dominant_carbon_source'])\n",
    "X_train = train_df[feature_cols + ['dominant_carbon_source']].copy()\n",
    "y_train = train_df[target_cols].copy()\n",
    "X_test = test_df[feature_cols + ['dominant_carbon_source']].copy()\n",
    "y_test = test_df[target_cols].copy()\n",
    "test_df.to_csv(os.path.join(OUT_DIR, \"test_set_for_verification.csv\"), index=False)\n",
    "\n",
    "# ----- PREPROCESSING -----\n",
    "numeric_features = [c for c in feature_cols if df[c].dtype in ['float64', 'int64']]\n",
    "categorical_features = ['dominant_carbon_source']\n",
    "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor = ColumnTransformer([('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# ----- MODEL TRAINING -----\n",
    "rf_pipe = Pipeline([('pre', preprocessor), ('model', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))])\n",
    "param_dist = {\n",
    "    'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rs = RandomizedSearchCV(rf_pipe, param_distributions=param_dist, n_iter=10, cv=3, scoring='r2', random_state=RANDOM_STATE, n_jobs=-1, verbose=1)\n",
    "print(\"\\nRunning RandomizedSearchCV...\")\n",
    "rs.fit(X_train, y_train)\n",
    "best_model = rs.best_estimator_\n",
    "joblib.dump(best_model, os.path.join(OUT_DIR, \"best_model.joblib\"))\n",
    "\n",
    "# ----- PREDICT ON TEST SET & METRICS -----\n",
    "y_pred = pd.DataFrame(best_model.predict(X_test), index=y_test.index, columns=y_test.columns)\n",
    "metrics = {}\n",
    "for col in y_test.columns:\n",
    "    metrics[col] = {'r2': r2_score(y_test[col], y_pred[col]), 'rmse': np.sqrt(mean_squared_error(y_test[col], y_pred[col]))}\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, \"test_metrics_per_target.csv\"))\n",
    "print(\"\\nTest Metrics for the New Model:\\n\", metrics_df)\n",
    "\n",
    "# ----- PERMUTATION IMPORTANCE -----\n",
    "perm = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "imp_ser = pd.Series(perm.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
    "imp_ser.to_csv(os.path.join(OUT_DIR, \"permutation_importances.csv\"))\n",
    "\n",
    "# ----- PLOTS -----\n",
    "plt.figure(figsize=(10, 8))\n",
    "top20 = imp_ser.head(20)\n",
    "sns.barplot(x=top20.values, y=top20.index)\n",
    "plt.title(\"Top 20 Most Important Features (New Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importances_top20.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "### --- ADDED SECTION: Diagnostic Plots --- ###\n",
    "print(\"\\nGenerating and organizing diagnostic plots...\")\n",
    "\n",
    "# 1. Residuals vs. Predicted Plots\n",
    "residual_output_dir = os.path.join(OUT_DIR, \"residual_plots\")\n",
    "os.makedirs(residual_output_dir, exist_ok=True)\n",
    "print(f\"Saving residual plots to: {residual_output_dir}\")\n",
    "for i, col in enumerate(y_test.columns):\n",
    "    residuals = y_test[col] - y_pred[col]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred[col], residuals, alpha=0.6)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "    plt.title(f\"Residuals vs. Predicted Plot for {col}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(residual_output_dir, f\"residuals_vs_predicted_{col}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "# 2. Learning Curve Plots\n",
    "lc_output_dir = os.path.join(OUT_DIR, \"learning_curves\")\n",
    "os.makedirs(lc_output_dir, exist_ok=True)\n",
    "print(f\"Saving learning curve plots to: {lc_output_dir}\")\n",
    "scoring_metrics = {'R-squared': 'r2', 'Mean Squared Error': 'neg_mean_squared_error'}\n",
    "for target_col in y_train.columns:\n",
    "    print(f\"Generating learning curve for: {target_col}...\")\n",
    "    fig, axes = plt.subplots(1, len(scoring_metrics), figsize=(12, 5), sharey=False)\n",
    "    fig.suptitle(f'Learning Curves for {target_col}', fontsize=16)\n",
    "    for i, (metric_name, metric_scorer) in enumerate(scoring_metrics.items()):\n",
    "        train_sizes, train_scores, validation_scores = learning_curve(\n",
    "            estimator=best_model, X=X_train, y=y_train[target_col],\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring=metric_scorer, n_jobs=-1)\n",
    "        if \"neg_\" in metric_scorer:\n",
    "            train_scores, validation_scores = -train_scores, -validation_scores\n",
    "        train_scores_mean, validation_scores_mean = np.mean(train_scores, axis=1), np.mean(validation_scores, axis=1)\n",
    "        ax = axes[i]\n",
    "        ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        ax.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        ax.set_title(metric_name)\n",
    "        ax.set_xlabel(\"Training Set Size\")\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.grid(True)\n",
    "    plot_filename = os.path.join(lc_output_dir, f\"lc_{target_col}.png\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… All outputs, plots and the model are saved in the '{OUT_DIR}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20af8dd-4c12-4a1a-88b0-e16870b44d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
