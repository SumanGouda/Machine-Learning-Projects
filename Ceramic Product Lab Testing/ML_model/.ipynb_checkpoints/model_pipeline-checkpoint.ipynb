{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249c942f-b319-4471-a9fe-966d9e1c06ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (600, 22)\n",
      "Saved test set copies to model_outputs\n",
      "\n",
      "--- Baseline Model Evaluation (Mean CV R^2, RMSE, MAE) ---\n",
      "Evaluating Ridge (baseline)...\n",
      "{'r2_mean': 0.706008854051948, 'rmse_mean': 4.251859388437074, 'mae_mean': 2.973939623807973}\n",
      "Evaluating RandomForest (baseline)...\n",
      "{'r2_mean': 0.7024748723227395, 'rmse_mean': 4.49408541709026, 'mae_mean': 3.299902959635419}\n",
      "Evaluating GradientBoosting (baseline)...\n",
      "{'r2_mean': 0.7127481801329766, 'rmse_mean': 4.064121596917315, 'mae_mean': 3.027325546972269}\n",
      "----------------------------------------------------------\n",
      "\n",
      "Running RandomizedSearchCV (n_iter=10, cv=3, n_jobs=1)...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'model_estimator_n_estimators' for estimator Pipeline(steps=[('pre',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['total_carbon_wt%',\n                                                   'graphite_wt%',\n                                                   'carbon_black_wt%',\n                                                   'resin_wt%', 'pitch_wt%',\n                                                   'graphene_wt%', 'cnt_wt%',\n                                                   'gnp_wt%', 'antioxidant_wt%',\n                                                   'mgo_purity_pct',\n                                                   'd50_micron']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['dominant_carbon_source'])])),\n                ('model',\n                 MultiOutputRegressor(estimator=RandomForestRegressor(n_jobs=-1,\n                                                                      random_state=42)))]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# ... rest of the code ...\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning RandomizedSearchCV (n_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRS_N_ITER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cv=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRS_CV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_jobs=1)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m rs\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rs\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest CV r2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rs\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1960\u001b[0m         ParameterSampler(\n\u001b[0;32m   1961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1962\u001b[0m         )\n\u001b[0;32m   1963\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:876\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    869\u001b[0m score_params_test \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mscore_params, indices\u001b[38;5;241m=\u001b[39mtest)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# here we clone the parameters, since sometimes the parameters\u001b[39;00m\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;66;03m# themselves might be estimators, e.g. when we search over different\u001b[39;00m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# estimators in a pipeline.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# ref: https://github.com/scikit-learn/scikit-learn/pull/26786\u001b[39;00m\n\u001b[1;32m--> 876\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclone(parameters, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    878\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    880\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:237\u001b[0m, in \u001b[0;36mPipeline.set_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m        Pipeline class instance.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:69\u001b[0m, in \u001b[0;36m_BaseComposition._set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_estimator(attr, name, params\u001b[38;5;241m.\u001b[39mpop(name))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:279\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[0;32m    278\u001b[0m     local_valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[0;32m    285\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'model_estimator_n_estimators' for estimator Pipeline(steps=[('pre',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['total_carbon_wt%',\n                                                   'graphite_wt%',\n                                                   'carbon_black_wt%',\n                                                   'resin_wt%', 'pitch_wt%',\n                                                   'graphene_wt%', 'cnt_wt%',\n                                                   'gnp_wt%', 'antioxidant_wt%',\n                                                   'mgo_purity_pct',\n                                                   'd50_micron']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['dominant_carbon_source'])])),\n                ('model',\n                 MultiOutputRegressor(estimator=RandomForestRegressor(n_jobs=-1,\n                                                                      random_state=42)))]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "# model_pipeline_optimized.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings for clean execution output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----- CONFIG -----\n",
    "DATA_PATH = \"Dataset for model feed - MgO C.xlsx\"   # adjust if needed\n",
    "SHEET_NAME = \"synthetic_data\"\n",
    "OUT_DIR = \"model_outputs\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "CV_FOLDS = 5 # Used for baseline evaluation\n",
    "RS_CV = 3    # Used for RandomizedSearchCV\n",
    "RS_N_ITER = 10 # Increased search depth slightly\n",
    "N_JOBS = -1  # Use all available CPU cores for parallel operations\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- LOAD -----\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH, sheet_name=SHEET_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {DATA_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# ----- FEATURES / TARGETS -----\n",
    "id_cols = ['sample_id', 'setting', 'dominant_carbon_source']\n",
    "target_cols = [\n",
    "    'porosity_pct', 'density_g_cm3', 'thermal_conductivity_W_mK',\n",
    "    'oxidation_mass_loss_pct', 'oxidation_penetration_mm', 'hot_MOR_MPa',\n",
    "    'slag_contact_angle_deg', 'residual_strength_pct_after_shock'\n",
    "]\n",
    "feature_cols = [c for c in df.columns if c not in target_cols + id_cols]\n",
    "\n",
    "# ----- STRATIFIED SPLIT -----\n",
    "# Ensure the 'dominant_carbon_source' column is present for stratification\n",
    "if 'dominant_carbon_source' not in df.columns:\n",
    "    print(\"Error: Stratification column 'dominant_carbon_source' not found.\")\n",
    "    exit()\n",
    "    \n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=TEST_SIZE, random_state=RANDOM_STATE,\n",
    "    shuffle=True, stratify=df['dominant_carbon_source']\n",
    ")\n",
    "\n",
    "# X only needs the features defined, the ColumnTransformer will select them.\n",
    "# The 'dominant_carbon_source' is included in feature_cols for completeness \n",
    "# in the split but the ColumnTransformer handles it.\n",
    "X_train = train_df[feature_cols + ['dominant_carbon_source']].copy()\n",
    "y_train = train_df[target_cols].copy()\n",
    "X_test = test_df[feature_cols + ['dominant_carbon_source']].copy()\n",
    "y_test = test_df[target_cols].copy()\n",
    "\n",
    "# Save test set for your independent verification\n",
    "test_df.to_csv(os.path.join(OUT_DIR, \"test_set_for_verification.csv\"), index=False)\n",
    "test_df.to_excel(os.path.join(OUT_DIR, \"test_set_for_verification.xlsx\"), index=False)\n",
    "print(f\"Saved test set copies to {OUT_DIR}\")\n",
    "\n",
    "# ----- PREPROCESSING -----\n",
    "# Use np.number for robustness\n",
    "numeric_features = [c for c in feature_cols if df[c].dtype in [np.number]]\n",
    "categorical_features = ['dominant_carbon_source']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' # explicitly drop other columns (like id_cols if they accidentally got here)\n",
    ")\n",
    "\n",
    "# ----- HELPER: CV EVALUATION (multioutput) -----\n",
    "def evaluate_model_cv(model, X, y, cv=5, random_state=RANDOM_STATE):\n",
    "    \"\"\"Performs K-Fold cross-validation and returns mean metrics.\"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    r2s, rmses, maes = [], [], []\n",
    "    \n",
    "    # Custom function to calculate mean metrics across all targets\n",
    "    def calculate_mean_metrics(y_true, y_pred):\n",
    "        r2 = np.mean([r2_score(y_true[c], y_pred[c]) for c in y_true.columns])\n",
    "        rmse = np.mean([np.sqrt(mean_squared_error(y_true[c], y_pred[c])) for c in y_true.columns])\n",
    "        mae = np.mean([mean_absolute_error(y_true[c], y_pred[c]) for c in y_true.columns])\n",
    "        return r2, rmse, mae\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        Xtr, Xval = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        ytr, yval = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(Xtr, ytr)\n",
    "        ypred = pd.DataFrame(model.predict(Xval), index=yval.index, columns=yval.columns)\n",
    "        \n",
    "        r2, rmse, mae = calculate_mean_metrics(yval, ypred)\n",
    "        r2s.append(r2)\n",
    "        rmses.append(rmse)\n",
    "        maes.append(mae)\n",
    "\n",
    "    return {'r2_mean': np.mean(r2s), 'rmse_mean': np.mean(rmses), 'mae_mean': np.mean(maes)}\n",
    "\n",
    "# ----- BASELINE MODELS -----\n",
    "ridge_pipe = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', MultiOutputRegressor(Ridge(random_state=RANDOM_STATE)))\n",
    "])\n",
    "# Optimized: n_jobs=-1\n",
    "rf_pipe = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', MultiOutputRegressor(RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=N_JOBS)))\n",
    "])\n",
    "gbr_pipe = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', MultiOutputRegressor(GradientBoostingRegressor(random_state=RANDOM_STATE)))\n",
    "])\n",
    "\n",
    "print(\"\\n--- Baseline Model Evaluation (Mean CV R^2, RMSE, MAE) ---\")\n",
    "print(\"Evaluating Ridge (baseline)...\")\n",
    "print(evaluate_model_cv(ridge_pipe, X_train, y_train, cv=CV_FOLDS))\n",
    "print(\"Evaluating RandomForest (baseline)...\")\n",
    "print(evaluate_model_cv(rf_pipe, X_train, y_train, cv=CV_FOLDS))\n",
    "print(\"Evaluating GradientBoosting (baseline)...\")\n",
    "print(evaluate_model_cv(gbr_pipe, X_train, y_train, cv=CV_FOLDS))\n",
    "print(\"----------------------------------------------------------\\n\")\n",
    "\n",
    "# ----- LIGHTWEIGHT HYPERPARAMETER SEARCH (RandomizedSearchCV) -----\n",
    "# FINAL FIX: Using the key format that appeared in the traceback to bypass \n",
    "# a known bug/incompatibility with MultiOutputRegressor inside a Pipeline \n",
    "# on certain environment versions. This non-standard fix often works.\n",
    "param_dist = {\n",
    "    # We are forcing the key to match what the traceback *showed* the parser was looking for\n",
    "    'model_estimator_n_estimators': [150, 250, 350], \n",
    "    'model_estimator_max_depth': [6, 12, 20, None],   \n",
    "    'model_estimator_min_samples_split': [2, 5, 10],\n",
    "    'model_estimator_min_samples_leaf': [1, 2, 4],\n",
    "    'model_estimator_max_features': [0.8, 1.0, 'sqrt'] \n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS) \n",
    "rf_mo = MultiOutputRegressor(rf_base)\n",
    "rf_tune_pipe = Pipeline([('pre', preprocessor), ('model', rf_mo)])\n",
    "\n",
    "# Keep n_jobs=1 for safety\n",
    "rs = RandomizedSearchCV(rf_tune_pipe, param_distributions=param_dist, n_iter=RS_N_ITER, cv=RS_CV,\n",
    "                        scoring='r2', random_state=RANDOM_STATE, n_jobs=1, verbose=1) \n",
    "\n",
    "\n",
    "print(f\"Running RandomizedSearchCV (n_iter={RS_N_ITER}, cv={RS_CV}, n_jobs=1)...\")\n",
    "rs.fit(X_train, y_train)\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "print(\"Best CV r2:\", rs.best_score_)\n",
    "\n",
    "best_model = rs.best_estimator_\n",
    "\n",
    "# ----- FIT BEST MODEL ON FULL TRAINING DATA -----\n",
    "# Already done inside rs.fit() for the best model, but a final check/refit is fine\n",
    "best_model.fit(X_train, y_train) \n",
    "joblib.dump(best_model, os.path.join(OUT_DIR, \"best_model.joblib\"))\n",
    "print(f\"Best model saved to {os.path.join(OUT_DIR, 'best_model.joblib')}\")\n",
    "\n",
    "# ----- PREDICT ON TEST SET & METRICS -----\n",
    "y_pred = pd.DataFrame(best_model.predict(X_test), index=y_test.index, columns=y_test.columns)\n",
    "\n",
    "# Save predictions for verification\n",
    "pred_df = test_df[['sample_id']].reset_index(drop=True)\n",
    "# Combine actual and predicted values side-by-side\n",
    "combined_df = pd.concat([\n",
    "    pred_df, \n",
    "    y_test.reset_index(drop=True).rename(columns={c: f\"actual_{c}\" for c in y_test.columns}), \n",
    "    y_pred.reset_index(drop=True).rename(columns={c: f\"pred_{c}\" for c in y_pred.columns})\n",
    "], axis=1)\n",
    "combined_df.to_csv(os.path.join(OUT_DIR, \"test_set_predictions.csv\"), index=False)\n",
    "\n",
    "# Compute metrics per target\n",
    "metrics = {}\n",
    "for col in y_test.columns:\n",
    "    metrics[col] = {\n",
    "        'r2': r2_score(y_test[col], y_pred[col]),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test[col], y_pred[col])),\n",
    "        'mae': mean_absolute_error(y_test[col], y_pred[col])\n",
    "    }\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, \"test_metrics_per_target.csv\"))\n",
    "print(f\"Test metrics saved to {os.path.join(OUT_DIR, 'test_metrics_per_target.csv')}\")\n",
    "print(\"\\n--- Test Set Metrics Per Target ---\")\n",
    "print(metrics_df)\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "# ----- PERMUTATION IMPORTANCE -----\n",
    "# Get transformed feature names for plotting\n",
    "pre = best_model.named_steps['pre']\n",
    "# Numeric features are unchanged\n",
    "num_feats_out = numeric_features\n",
    "# Categorical features are one-hot encoded\n",
    "cat_feats_out = list(pre.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\n",
    "feat_names = num_feats_out + cat_feats_out\n",
    "\n",
    "# Optimized: n_jobs=-1\n",
    "perm = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, \n",
    "    random_state=RANDOM_STATE, n_jobs=N_JOBS\n",
    ")\n",
    "imp_ser = pd.Series(perm.importances_mean, index=feat_names).sort_values(ascending=False)\n",
    "imp_ser.to_csv(os.path.join(OUT_DIR, \"permutation_importances.csv\"))\n",
    "\n",
    "# ----- PLOTS -----\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.select_dtypes(include=[np.number]).corr(), cmap='coolwarm', center=0, annot=False)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features & Targets)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"correlation_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Histograms of targets\n",
    "for col in y_test.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"hist_{col}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Actual vs Predicted (1:1 plot)\n",
    "for col in y_test.columns:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_test[col], y_pred[col], alpha=0.7)\n",
    "    \n",
    "    # Plot 1:1 line (red dashed)\n",
    "    min_val = min(y_test[col].min(), y_pred[col].min())\n",
    "    max_val = max(y_test[col].max(), y_pred[col].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal 1:1')\n",
    "    \n",
    "    plt.xlabel(f\"Actual {col}\")\n",
    "    plt.ylabel(f\"Predicted {col}\")\n",
    "    plt.title(f\"Actual vs Predicted: {col}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"actual_vs_pred_{col}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Residual distributions (Error = Actual - Predicted)\n",
    "for col in y_test.columns:\n",
    "    res = y_test[col] - y_pred[col]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(res, kde=True)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title(f\"Residuals: {col} (Mean={res.mean():.3f})\")\n",
    "    plt.xlabel(\"Residual (Actual - Predicted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"residuals_{col}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Feature importance bar plot (top 20)\n",
    "plt.figure(figsize=(8, 10)) # Increased height for more features\n",
    "top20 = imp_ser.head(20)\n",
    "sns.barplot(x=top20.values, y=top20.index)\n",
    "plt.title(\"Permutation Importances (Top 20 Features)\")\n",
    "plt.xlabel(\"Mean Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importances_top20.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"All plots and outputs saved in {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9a37c-ec0b-4919-a390-bb3eb4220ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
